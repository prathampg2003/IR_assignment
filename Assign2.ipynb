{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CSD358 - Information Retrieval\n",
        "### Assignment 2\n",
        "#### Tejaswi Manavala Narayanan - 2110110556\n",
        "#### Pratham Goel - 2110110388"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Github link: https://github.com/prathampg2003/IR_assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "qzXfN-V-xzqS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import re\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "LUDtPyj6xzqV"
      },
      "outputs": [],
      "source": [
        "def  read_file(fname):\n",
        "    with open(fname, 'r', encoding = 'utf8') as f:\n",
        "        txt = f.read()\n",
        "        return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\python312\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\python312\\lib\\site-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "jiP4FrN9xzqW",
        "outputId": "e76200c1-51d1-4b61-96a7-7a7e5c6aa8af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mdtej\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\mdtej\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\mdtej\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Ob3Op7cjxzqX",
        "outputId": "89b37ab5-d3fe-470f-c534-21337422e724"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\mdtej\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\mdtej\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "nltk.download(\"punkt\")\n",
        "ps = PorterStemmer()\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDBfrGWnxzqY"
      },
      "source": [
        "### Reading files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Space Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VectorSpaceModel:\n",
        "    def __init__(self):\n",
        "        self.dictionary = {}  # Stores term -> (df, [(doc_id, term_freq)])\n",
        "        self.doc_lengths = {}  # Stores doc_id -> document length\n",
        "        self.doc_count = 0     # Number of documents in the corpus\n",
        "        self.doc_id_to_file = {}  # Maps doc_id to file name\n",
        "        self.stop_words = set(stopwords.words('english'))  # Stop words set\n",
        "        self.lemmatizer = WordNetLemmatizer()  # Initialize lemmatizer\n",
        "        self.stemmer = PorterStemmer()  # Initialize stemmer\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"\n",
        "        Tokenizes, removes stop words, lemmatizes, and stems the text.\n",
        "        \"\"\"\n",
        "        tokens = re.findall(r'\\b\\w+\\b', text.lower())  # Tokenize and remove non-alphanumeric characters\n",
        "        filtered_tokens = [word for word in tokens if word not in self.stop_words]  # Remove stop words\n",
        "        lemmatized_tokens = [self.lemmatizer.lemmatize(word) for word in filtered_tokens]  # Lemmatization\n",
        "        stemmed_tokens = [self.stemmer.stem(word) for word in lemmatized_tokens]  # Stemming\n",
        "        return stemmed_tokens\n",
        "\n",
        "    def process_corpus(self, directory):\n",
        "        \"\"\"\n",
        "        Reads documents from the corpus directory and preprocesses them.\n",
        "        \"\"\"\n",
        "        allfiles = {}\n",
        "        for i, file in enumerate(os.listdir(directory), 1):\n",
        "            with open(os.path.join(directory, file), 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "            preprocessed_content = self.preprocess(content)\n",
        "            allfiles[i] = preprocessed_content\n",
        "            self.doc_id_to_file[i] = file\n",
        "            self.add_document(i, preprocessed_content)  # Add document terms to index\n",
        "        return allfiles\n",
        "\n",
        "    def add_document(self, doc_id, terms):\n",
        "        \"\"\"\n",
        "        Adds a document's terms to the index.\n",
        "        \"\"\"\n",
        "        term_freqs = defaultdict(int)\n",
        "        for term in terms:\n",
        "            term_freqs[term] += 1\n",
        "\n",
        "        for term, tf in term_freqs.items():\n",
        "            if term not in self.dictionary:\n",
        "                self.dictionary[term] = (0, [])\n",
        "            df, postings = self.dictionary[term]\n",
        "            postings.append((doc_id, tf))\n",
        "            self.dictionary[term] = (df + 1, postings)\n",
        "\n",
        "        # Calculate and store document length (used for normalization)\n",
        "        doc_length = math.sqrt(sum((1 + math.log10(tf))**2 for tf in term_freqs.values()))\n",
        "        self.doc_lengths[doc_id] = doc_length\n",
        "        self.doc_count += 1\n",
        "\n",
        "    def tf_idf(self, term, doc_id, tf, for_query=False):\n",
        "        \"\"\"\n",
        "        Computes the tf-idf value for a term in a document or query.\n",
        "        \"\"\"\n",
        "        df, _ = self.dictionary[term]\n",
        "        idf = math.log10(self.doc_count / df) if df > 0 else 0\n",
        "        log_tf = 1 + math.log10(tf) if tf > 0 else 0\n",
        "\n",
        "        if for_query:\n",
        "            return log_tf * idf  # Use idf for query\n",
        "        return log_tf  # no idf for documents\n",
        "\n",
        "    def rank_documents(self, query_terms):\n",
        "            \"\"\"\n",
        "            Ranks documents by cosine similarity between the query and each document.\n",
        "            \"\"\"\n",
        "            query_term_freqs = defaultdict(int)\n",
        "            \n",
        "            # Count term frequencies in the query\n",
        "            for term in query_terms:\n",
        "                if term in self.dictionary:\n",
        "                    query_term_freqs[term] += 1\n",
        "\n",
        "            # Create the query vector and apply log tf and idf weighting\n",
        "            query_vector = {}\n",
        "            for term, tf in query_term_freqs.items():\n",
        "                query_vector[term] = self.tf_idf(term, None, tf, for_query=True)\n",
        "\n",
        "            # Normalize the query vector\n",
        "            query_length = math.sqrt(sum(weight ** 2 for weight in query_vector.values()))\n",
        "            \n",
        "            if query_length > 0:\n",
        "                # Normalize each weight in the query vector\n",
        "                query_vector = {term: weight / query_length for term, weight in query_vector.items()}\n",
        "\n",
        "            # Calculate cosine similarity for each document\n",
        "            scores = defaultdict(float)\n",
        "            \n",
        "            for term, query_weight in query_vector.items():\n",
        "                df, postings = self.dictionary.get(term, (0, []))\n",
        "                for doc_id, tf in postings:\n",
        "                    # Calculate document weight using TF-IDF\n",
        "                    doc_weight = self.tf_idf(term, doc_id, tf)\n",
        "                    scores[doc_id] += query_weight * doc_weight\n",
        "\n",
        "            # Normalize document scores using cosine normalization\n",
        "            for doc_id in scores:\n",
        "                doc_vector_length = self.doc_lengths[doc_id]  # We already store the Euclidean length\n",
        "                \n",
        "                if doc_vector_length > 0:\n",
        "                    scores[doc_id] /= doc_vector_length  # Apply correct cosine normalization\n",
        "\n",
        "            ranked_docs = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n",
        "            ranked_filenames = [(self.doc_id_to_file[doc_id], score) for doc_id, score in ranked_docs[:10]]\n",
        "\n",
        "            return ranked_filenames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top relevant documents for query 'Developing your Zomato business account and profile is a great way to boost your restaurant's online reputation':\n",
            "zomato.txt: 0.21460251\n",
            "swiggy.txt: 0.13100810\n",
            "instagram.txt: 0.06052477\n",
            "messenger.txt: 0.05916808\n",
            "youtube.txt: 0.05845097\n",
            "Discord.txt: 0.05331835\n",
            "bing.txt: 0.05177956\n",
            "paypal.txt: 0.04708566\n",
            "reddit.txt: 0.04409441\n",
            "flipkart.txt: 0.04072831\n"
          ]
        }
      ],
      "source": [
        "vsm = VectorSpaceModel()\n",
        "corpus_directory = \"Corpus\"\n",
        "vsm.process_corpus(corpus_directory)\n",
        "\n",
        "# Search with a free-text query\n",
        "query = \"Developing your Zomato business account and profile is a great way to boost your restaurant's online reputation\"\n",
        "query_terms = vsm.preprocess(query)\n",
        "result = vsm.rank_documents(query_terms)\n",
        "\n",
        "# Print the top relevant documents with scores\n",
        "print(f\"Top relevant documents for query '{query}':\")\n",
        "for i in range(10):\n",
        "    print(f\"{result[i][0]}: {result[i][1]:.8f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top relevant documents for query 'Warwickshire, came from an ancient family and was the heiress to some land':\n",
            "shakespeare.txt: 0.11997620\n",
            "levis.txt: 0.02414239\n",
            "Adobe.txt: 0.02265058\n",
            "google.txt: 0.02072642\n",
            "nike.txt: 0.01921104\n",
            "zomato.txt: 0.01770312\n",
            "huawei.txt: 0.01372434\n",
            "skype.txt: 0.01170110\n",
            "blackberry.txt: 0.01094421\n",
            "Dell.txt: 0.01076635\n"
          ]
        }
      ],
      "source": [
        "query = \"Warwickshire, came from an ancient family and was the heiress to some land\"\n",
        "query_terms = vsm.preprocess(query)\n",
        "result = vsm.rank_documents(query_terms)\n",
        "\n",
        "# Print the top relevant documents with scores\n",
        "print(f\"Top relevant documents for query '{query}':\")\n",
        "for i in range(10):\n",
        "    print(f\"{result[i][0]}: {result[i][1]:.8f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
