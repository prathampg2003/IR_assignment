{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qzXfN-V-xzqS"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LUDtPyj6xzqV"
      },
      "outputs": [],
      "source": [
        "def  read_file(fname):\n",
        "    with open(fname, 'r', encoding = 'utf8') as f:\n",
        "        txt = f.read()\n",
        "        return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: click in c:\\users\\prath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\prath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\users\\prath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\prath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.1/1.5 MB 1.6 MB/s eta 0:00:01\n",
            "   ---- ----------------------------------- 0.2/1.5 MB 2.1 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 0.4/1.5 MB 3.4 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 0.8/1.5 MB 5.0 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 1.2/1.5 MB 6.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  1.5/1.5 MB 6.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.5/1.5 MB 5.6 MB/s eta 0:00:00\n",
            "Installing collected packages: nltk\n",
            "Successfully installed nltk-3.9.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jiP4FrN9xzqW",
        "outputId": "e76200c1-51d1-4b61-96a7-7a7e5c6aa8af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\prath\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ob3Op7cjxzqX",
        "outputId": "89b37ab5-d3fe-470f-c534-21337422e724"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\prath\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\prath\\AppData\\Roaming\\nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "nltk.download(\"punkt\")\n",
        "ps = PorterStemmer()\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDBfrGWnxzqY"
      },
      "source": [
        "### Reading files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IakPVgT6xzqa"
      },
      "outputs": [],
      "source": [
        "allfiles = {}\n",
        "i = 1\n",
        "for file in os.listdir('Corpus'):\n",
        "    allfiles[i] = read_file('Corpus/'+file)\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what is adobe?\n",
            "\n",
            "The company was founded in 1982 by John Warnock and Charles Geschke. While employed at Xerox Corporation’s Palo Alto (California) Research Center (PARC), the two computer scientists had developed a programming language specially designed to describe the precise position, shape, and size of objects on a computer-generated page. This page description language, later known as PostScript, described such objects as letters and graphics in mathematical terms, without reference to any specific computer or printer; any device capable of interpreting the language would be able to generate a representation of the page at any resolution the device supported. When Xerox declined to bring the technology to market, Warnock and Geschke formed their own company to do so, naming it after a creek near their homes.\n",
            "\n",
            "In 1983 Apple Computer, Inc. (now Apple Inc.), acquired 15 percent of Adobe and became the first licensee of PostScript. In 1985 Apple introduced the first Macintosh-compatible PostScript printer, the LaserWriter, based on a laser-print engine developed by Canon Inc. The LaserWriter included PostScript renditions of several classic typefaces and a PostScript interpreter—in effect, a built-in computer dedicated to the task of translating PostScript commands into marks on each page.\n",
            "\n",
            "Compared with the printing options previously available for personal computers, the combination of PostScript and laser printing represented a dramatic advance in typographical quality and design flexibility. Together with PageMaker, a page-layout application developed by Aldus Corporation, these technologies enabled any computer user to produce professional-looking reports, flyers, and newsletters without specialized lithography equipment and training—a phenomenon that became known as desktop publishing. Adobe made its initial public stock offering in 1986. Although revenues grew to $168.7 million by 1990, Adobe’s relations with Apple deteriorated in the late 1980s over PostScript licensing fees, and in 1989 Apple announced plans to sell its Adobe stock, collaborate with Microsoft Corporation on development of an enhanced PostScript clone, and introduce a new font-rendering technology of its own, called TrueType. For more than a year the dispute, known as the font wars, roiled the computer and publishing\n",
            "\n",
            "worlds before Apple and Adobe reached a compromise. In the wake of the agreement, Microsoft abandoned its PostScript clone and adopted TrueType for its Windows operating systems. During the 1990s Adobe’s revenues from PostScript licensing continued to increase, as did its sales of PostScript fonts; by the end of 1998 the Adobe Type Library encompassed more than 2,500 typefaces. An ever-larger share of the company’s revenues, however, came from sales of application software, initially for the Macintosh platform but later also for the UNIX and Windows operating systems. The first such application, introduced in 1987, was Adobe Illustrator, a PostScript-based drawing package for artists, designers, and technical illustrators. Adobe Photoshop, an application for retouching digitized photographic images, followed three years later and quickly became Adobe’s most successful program. It was one of the first commercial applications with an interface enabling outside developers to make new features available through \n",
            "\n",
            "plug-ins within the main program; scores of developers took advantage of this “open architecture,” helping to cement Photoshop’s dominance within its category.\n",
            "\n",
            "In subsequent years Adobe added many other applications, primarily through a series of acquisitions. In 1991 it brought out Adobe Premiere, a program for editing video and multimedia productions. In 1994 the company acquired Aldus and its PageMaker software. The following year Adobe bought Frame Technology Corporation, developer of FrameMaker, a program designed for the production of technical manuals and book-length documents. It also purchased Ceneca Communications, Inc., creator of PageMill, a program for creating World Wide Web pages, and SiteMill, a Web site-management utility. In 1996 Adobe released its first title aimed at consumers, a simplified photo-editing program called PhotoDeluxe.\n",
            "\n",
            "Another major company initiative in the 1990s—the Adobe Acrobat product family—was designed to provide a standard format for electronic document distribution. Once a document had been converted to Acrobat’s portable document format (PDF), regardless of its origins, users of any major computer operating system could read and print it, with formatting, typography, and graphics nearly intact, via the Acrobat Reader, an application the company offered for free. However, with the advent of the Internet and its need for compact file transfers, a competing format, HyperText Markup Language (HTML), eroded its market. In 1998 PDF became the new image format for the Macintosh operating system.\n",
            "\n",
            "By 1997 almost 80 percent of Adobe’s total revenues came from application sales, and that year marked the first time that Windows-product revenues exceeded Macintosh-product revenues. In 2005 Adobe acquired Macromedia, Inc. In addition to Macromedia FreeHand (a major competitor of Illustrator), Dreamweaver (Web-authoring software), and Director (software for producing CD-ROMs), Adobe gained two innovative programs, Shockwave and Flash, for producing and distributing animations and interactive media over the Internet for viewing in Web browsers. In 2008 Adobe Media Player was introduced as a competitor to Apple’s iTunes, Windows Media Player, and RealPlayer from RealNetworks, Inc. In addition to playing audio and video files in a variety of formats on personal computers, Adobe Media Player was adopted by several television networks for deploying television shows over the Internet in the highly compact Flash format.\n"
          ]
        }
      ],
      "source": [
        "print(allfiles[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fik7ovDoxzqa"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nhUP2Oexxzqa"
      },
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\prath/nltk_data'\n    - 'c:\\\\Users\\\\prath\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\prath\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\prath\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\prath\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m42\u001b[39m):\n\u001b[0;32m      6\u001b[0m     allfiles[i] \u001b[38;5;241m=\u001b[39m allfiles[i]\u001b[38;5;241m.\u001b[39mlower()    \u001b[38;5;66;03m# case folding\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     allfiles[i] \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# sentence segmentation\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(allfiles[i])\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(allfiles[i])):\n",
            "File \u001b[1;32mc:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
            "File \u001b[1;32mc:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
            "File \u001b[1;32mc:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\prath/nltk_data'\n    - 'c:\\\\Users\\\\prath\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\prath\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\prath\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\prath\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "doclist = {}\n",
        "for i in range(1,42):\n",
        "    doclist[i] = []\n",
        "\n",
        "for i in range(1,42):\n",
        "    allfiles[i] = allfiles[i].lower()    # case folding\n",
        "    allfiles[i] = sent_tokenize(allfiles[i])    # sentence segmentation\n",
        "    # print(allfiles[i])\n",
        "    for j in range(len(allfiles[i])):\n",
        "        allfiles[i][j] = word_tokenize(allfiles[i][j])    # word tokenization\n",
        "        allfiles[i][j] = [word for word in allfiles[i][j] if word not in stop_words and word.isalpha()]    # stopword removal\n",
        "        # allfiles[i][j] = [ps.stem(word) for word in allfiles[i][j]]   # stemming\n",
        "        allfiles[i][j] = [lemmatizer.lemmatize(word) for word in allfiles[i][j]]    # lemmatization\n",
        "        doclist[i] = doclist[i] + [k for k in allfiles[i][j]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFMUYs33xzqb"
      },
      "outputs": [],
      "source": [
        "# mapping docID to docName\n",
        "docID = {}\n",
        "k = 1\n",
        "for i in os.listdir('Corpus'):\n",
        "    docID[k] = i.replace('.txt', '')\n",
        "    k += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsVknwimxzqc",
        "outputId": "cc082bc3-ede4-4a8a-eeb0-0813aea955c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['restaurant', 'owner', 'marketing', 'manager', 'restaurant', 'love', 'zomato', 'zomato', 'exactly', 'would', 'love', 'introduce', 'platform', 'set', 'business', 'account', 'everything', 'zomato', 'offer', 'help', 'boost', 'business', 'zomato', 'zomato', 'around', 'since', 'acquired', 'urbanspoon', 'along', 'technology', 'provides', 'potential', 'customer', 'database', 'restaurant', 'discover', 'local', 'eatery', 'view', 'photo', 'menu', 'review', 'get', 'idea', 'restaurant', 'offer', 'customer', 'also', 'make', 'reservation', 'order', 'takeout', 'leave', 'review', 'zomato', 'provides', 'product', 'table', 'management', 'online', 'ordering', 'business', 'along', 'advertising', 'merchant', 'tool', 'built', 'help', 'restaurant', 'thrive', 'online', 'marketspace', 'connecting', 'customer', 'restaurant', 'zomato', 'main', 'goal', 'work', 'ensure', 'customer', 'best', 'experience', 'restaurant', 'found', 'platform', 'developing', 'zomato', 'business', 'account', 'profile', 'great', 'way', 'boost', 'restaurant', 'online', 'reputation', 'claim', 'restaurant', 'create', 'new', 'listing', 'zomato', 'like', 'every', 'platform', 'first', 'need', 'check', 'see', 'restaurant', 'already', 'listing', 'zomato', 'find', 'start', 'claiming', 'restaurant', 'otherwise', 'begin', 'creating', 'new', 'listing', 'instead', 'claim', 'start', 'listing', 'first', 'visit', 'zomato', 'business', 'homepage', 'search', 'restaurant', 'find', 'listing', 'add', 'restaurant', 'make', 'sure', 'business', 'search', 'page', 'search', 'engine', 'listing', 'zomato', 'unclaimed', 'see', 'option', 'claim', 'green', 'button', 'already', 'claimed', 'need', 'track', 'owner', 'page', 'contact', 'zomato', 'help', 'filippi', 'pizza', 'grotto', 'little', 'italy', 'yet', 'claimed', 'listing', 'restaurant', 'owner', 'would', 'click', 'green', 'outlined', 'claim', 'restaurant', 'button', 'order', 'claim', 'listing', 'need', 'verification', 'owner', 'manager', 'business', 'zomato', 'asks', 'document', 'proof', 'like', 'certificate', 'registration', 'representative', 'zomato', 'confirm', 'ownership', 'assist', 'next', 'step', 'submitted', 'proof', 'ownership', 'zomato', 'verify', 'claim', 'sending', 'sm', 'message', 'phone', 'number', 'provided', 'add', 'code', 'zomato', 'website', 'finish', 'claim', 'set', 'business', 'owner', 'account', 'next', 'create', 'zomato', 'account', 'providing', 'full', 'name', 'email', 'address', 'password', 'facebook', 'google', 'also', 'option', 'login', 'well', 'add', 'restaurant', 'zomato', 'account', 'add', 'newly', 'created', 'claimed', 'listing', 'zomato', 'account', 'filling', 'add', 'restaurant', 'form', 'enter', 'restaurant', 'name', 'city', 'location', 'phone', 'number', 'also', 'see', 'spot', 'asks', 'open', 'business', 'optimize', 'zomato', 'restaurant', 'listing', 'time', 'make', 'profile', 'robust', 'several', 'optional', 'piece', 'information', 'provide', 'help', 'people', 'find', 'restaurant', 'essential', 'want', 'business', 'stand', 'competitive', 'landscape', 'people', 'looking', 'place', 'eat', 'zomato', 'use', 'filter', 'narrow', 'search', 'exactly', 'looking', 'like', 'open', 'hour', 'cuisine', 'type', 'free', 'zomato', 'search', 'rochester', 'new', 'york', 'show', 'dogtown', 'profile', 'rating', 'vote', 'multiple', 'photo', 'food', 'offering', 'help', 'entice', 'customer', 'come', 'location', 'additional', 'field', 'fill', 'zomato', 'listing', 'alcohol', 'availability', 'service', 'lunch', 'dinner', 'nightlife', 'etc', 'seating', 'availability', 'available', 'specify', 'whether', 'indoor', 'outdoor', 'payment', 'option', 'cuisine', 'type', 'miscellaneous', 'tag', 'free', 'parking', 'entry', 'free', 'atm', 'etc', 'hour', 'operation', 'contact', 'information', 'restaurant', 'email', 'phone', 'number', 'zomato', 'list', 'guideline', 'help', 'restaurant', 'determine', 'edit', 'listing', 'explains', 'importance', 'business', 'name', 'address', 'restaurant', 'feature', 'hour', 'photo', 'menu', 'zomato', 'restaurant', 'profile', 'included', 'need', 'know', 'claimed', 'business', 'zomato', 'create', 'overview', 'profile', 'page', 'user', 'land', 'find', 'restaurant', 'view', 'photo', 'contact', 'info', 'opening', 'hour', 'address', 'average', 'cost', 'website', 'profile', 'even', 'includes', 'specific', 'element', 'business', 'like', 'cuisine', 'offer', 'breakfast', 'free', 'wifi', 'restaurant', 'profile', 'button', 'user', 'navigate', 'quickly', 'zomato', 'choose', 'thing', 'like', 'bookmark', 'order', 'customer', 'also', 'view', 'download', 'menu', 'great', 'ultimately', 'use', 'order', 'food', 'however', 'enable', 'functionality', 'within', 'zomato', 'profile', 'visitor', 'also', 'able', 'rate', 'restaurant', 'write', 'review', 'review', 'important', 'aspect', 'zomato', 'profile', 'since', 'impact', 'online', 'review', 'powerful', 'come', 'buying', 'decision', 'fact', 'survey', 'podium', 'show', 'clearly', 'review', 'extremely', 'important', 'business', 'willing', 'pay', 'something', 'sure', 'better', 'experience', 'say', 'online', 'review', 'impact', 'purchasing', 'decision', 'even', 'aware', 'review', 'zomato', 'profile', 'important', 'bring', 'marketing', 'strategy', 'need', 'address', 'negative', 'review', 'reward', 'positive', 'one', 'improve', 'offering', 'based', 'customer', 'feedback', 'one', 'feature', 'zomato', 'restaurant', 'profile', 'includes', 'sidebar', 'nearby', 'nearby', 'establishment', 'listed', 'alongside', 'average', 'review', 'rating', 'another', 'reason', 'boosting', 'average', 'score', 'remain', 'priority', 'nearby', 'restaurant', 'widget', 'show', 'list', 'restaurant', 'zomato', 'rating', 'range', 'zomato', 'product', 'business', 'zomato', 'offer', 'multitude', 'complementary', 'product', 'business', 'owner', 'grow', 'reach', 'build', 'online', 'reputation', 'help', 'equip', 'restaurant', 'thing', 'like', 'ad', 'banner', 'integration', 'landing', 'page', 'facebook', 'ordering', 'booking', 'service', 'po', 'system', 'reward', 'program', 'analytics', 'tracking', 'tool', 'advertising', 'zomato', 'zomato', 'claim', 'every', 'marketing', 'dollar', 'spent', 'zomato', 'return', 'million', 'monthly', 'visit', 'hard', 'believe', 'result', 'restaurant', 'advertising', 'country', 'platform', 'advertise', 'zomato', 'using', 'ad', 'banner', 'desktop', 'mobile', 'additional', 'exposure', 'also', 'given', 'collection', 'zomato', 'analytics', 'help', 'track', 'performance', 'ad', 'campaign', 'find', 'reaching', 'even', 'additional', 'tool', 'measure', 'track', 'incoming', 'call', 'recording', 'listening', 'functionality', 'website', 'widget', 'facebook', 'intgration', 'zomato', 'business', 'widget', 'add', 'website', 'informing', 'customer', 'zomato', 'might', 'prompt', 'leave', 'review', 'check', 'social', 'proof', 'verify', 'claim', 'website', 'also', 'widget', 'accept', 'reservation', 'website', 'encourage', 'interaction', 'bookmark', 'inform', 'visitor', 'multiple', 'location', 'zomato', 'simple', 'integration', 'facebook', 'well', 'add', 'tab', 'business', 'facebook', 'page', 'visitor', 'see', 'zomato', 'review', 'zomato', 'order', 'similar', 'grubhub', 'doordash', 'uber', 'eats', 'zomato', 'offer', 'food', 'takeout', 'delivery', 'service', 'using', 'reduces', 'number', 'click', 'potential', 'customer', 'looking', 'review', 'ordering', 'restaurant', 'simplifying', 'transition', 'customer', 'evaluation', 'decision', 'stage', 'best', 'interest', 'le', 'likely', 'choose', 'competitor', 'along', 'way', 'simply', 'get', 'distracted', 'accepting', 'order', 'zomato', 'super', 'simple', 'time', 'receive', 'order', 'order', 'management', 'device', 'flash', 'beep', 'tap', 'accept', 'set', 'expected', 'delivery', 'time', 'accordingly', 'order', 'leaving', 'restaurant', 'tap', 'way', 'let', 'customer', 'know', 'zomato', 'trace', 'zomato', 'trace', 'perfect', 'pair', 'zomato', 'order', 'delivery', 'service', 'give', 'advanced', 'control', 'accuracy', 'delivery', 'time', 'estimate', 'trace', 'includes', 'delivery', 'tracking', 'live', 'traffic', 'condition', 'report', 'ability', 'replay', 'past', 'trip', 'see', 'driver', 'location', 'service', 'integrated', 'po', 'order', 'system', 'mobile', 'app', 'also', 'use', 'internal', 'delivery', 'management', 'application', 'provide', 'driver', 'route', 'zomato', 'business', 'app', 'zomato', 'business', 'mobile', 'app', 'allows', 'take', 'control', 'business', 'listing', 'expansive', 'powerful', 'way', 'manage', 'review', 'review', 'new', 'photo', 'uploads', 'update', 'listing', 'information', 'check', 'analytical', 'data', 'create', 'promotion', 'post', 'event', 'profile', 'customer', 'find', 'also', 'space', 'add', 'seasonal', 'special', 'menu', 'food', 'work', 'food', 'work', 'app', 'owned', 'zomato', 'make', 'seamless', 'mobile', 'experience', 'perfect', 'food', 'truck', 'vendor', 'often', 'serve', 'festival', 'large', 'event', 'app', 'customer', 'order', 'ahead', 'time', 'skip', 'long', 'line', 'use', 'kiosk', 'zomato', 'gold', 'zomato', 'allows', 'user', 'join', 'exclusive', 'club', 'loyalty', 'program', 'subscription', 'fee', 'called', 'zomato', 'gold', 'join', 'customer', 'visit', 'restaurant', 'earn', 'gold', 'show', 'unique', 'gold', 'id', 'member', 'staff', 'visit', 'restaurant', 'participate', 'gold', 'program', 'usually', 'offer', 'customer', 'sort', 'perk', 'like', 'complimentary', 'drink', 'meal', 'trending', 'restaurant', 'zomato', 'homepage', 'business', 'get', 'lot', 'review', 'weekly', 'basis', 'maintains', 'positive', 'average', 'could', 'end', 'trending', 'zomato', 'homepage', 'highly', 'competitive', 'space', 'occupy', 'however', 'requires', 'dedication', 'energy', 'get', 'customer', 'leave', 'review', 'engage', 'brand', 'zomato', 'visit', 'additional', 'consideration', 'tool', 'important', 'understand', 'exactly', 'people', 'use', 'zomato', 'similar', 'platform', 'knowing', 'someone', 'us', 'technology', 'find', 'business', 'like', 'help', 'understand', 'best', 'reach', 'addition', 'external', 'review', 'listing', 'website', 'like', 'zomato', 'google', 'yelp', 'facebook', 'people', 'also', 'turn', 'website', 'information', 'restaurant', 'include', 'online', 'review', 'one', 'place', 'little', 'reason', 'return', 'website', 'keeping', 'visitor', 'website', 'allows', 'control', 'experience', 'customer', 'also', 'boost', 'seo', 'giving', 'weight', 'website', 'relevancy', 'people', 'site', 'longer', 'visit', 'page', 'try', 'including', 'zomato', 'review', 'review', 'collection', 'yet', 'added', 'review', 'website', 'consider', 'building', 'website', 'review', 'convert', 'customer']\n"
          ]
        }
      ],
      "source": [
        "print(doclist[41])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_QAhRiHxzqc"
      },
      "source": [
        "### Creating Inverted Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8pyeXSRxzqc"
      },
      "outputs": [],
      "source": [
        "# creating inverted index\n",
        "inverted_index = {}\n",
        "for i in range(1,42):\n",
        "    for j in range(len(doclist[i])):\n",
        "        if doclist[i][j] not in inverted_index:\n",
        "            inverted_index[doclist[i][j]] = []\n",
        "        if i not in inverted_index[doclist[i][j]]:\n",
        "            inverted_index[doclist[i][j]].append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVKlCbUlxzqd",
        "outputId": "b644055d-401e-4fa8-cc4e-b0356f6b0f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Adobe', 'Amazon', 'apple', 'Binance', 'Dell', 'google', 'HP', 'huawei', 'Lenovo', 'messenger', 'microsoft', 'motorola', 'nike', 'nokia', 'Ola', 'operating', 'samsung', 'skype', 'sony', 'steam', 'Uber', 'zomato']\n",
            "['blackberry', 'google', 'huawei', 'instagram', 'messenger', 'nokia', 'operating', 'skype', 'telegram', 'whatsapp', 'zomato']\n"
          ]
        }
      ],
      "source": [
        "print([docID[j] for j in inverted_index['technology']])\n",
        "print([docID[j] for j in inverted_index['phone']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btALZPIIxzqd"
      },
      "source": [
        "### Handling Queries - Boolean Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4C2Qwmlxzqe"
      },
      "outputs": [],
      "source": [
        "# getting respective posting list\n",
        "def get_posting(term):\n",
        "    if term in inverted_index:\n",
        "        return set(inverted_index[term])\n",
        "    return set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpjFwRA-xzqe"
      },
      "outputs": [],
      "source": [
        "def evaluate_and(terms):\n",
        "    # Use parallel pointer for AND\n",
        "    if len(terms) == 0:\n",
        "        return set()\n",
        "    result = terms[0]\n",
        "    for term in terms[1:]:\n",
        "        result = result.intersection(term)\n",
        "    return result\n",
        "\n",
        "def evaluate_or(terms):\n",
        "    # Use parallel pointer for OR\n",
        "    result = set()\n",
        "    for term in terms:\n",
        "        result = result.union(term)\n",
        "    return result\n",
        "\n",
        "def evaluate_not(term):\n",
        "    # Use NOT by subtracting from all documents\n",
        "    all_docs = set(range(1, 41))  # Assuming documents are numbered 1 to 40\n",
        "    return all_docs - term\n",
        "\n",
        "# Evaluate the query using a stack-based approach\n",
        "def evaluate(ops, terms):\n",
        "    op = ops.pop()\n",
        "    if op == 'and':\n",
        "        term2 = terms.pop()\n",
        "        term1 = terms.pop()\n",
        "        terms.append(evaluate_and([term1, term2]))\n",
        "    elif op == 'or':\n",
        "        term2 = terms.pop()\n",
        "        term1 = terms.pop()\n",
        "        terms.append(evaluate_or([term1, term2]))\n",
        "    elif op == 'not':\n",
        "        term = terms.pop()\n",
        "        terms.append(evaluate_not(term))\n",
        "\n",
        "# Boolean query processing function\n",
        "def process_boolean_query(query):\n",
        "    # Tokenize the query\n",
        "    query = query.lower()  # case folding for the query as well\n",
        "    # query = word_tokenize(query)\n",
        "    query =' '.join([lemmatizer.lemmatize(i) for i in query.split()])\n",
        "    tokens = re.findall(r'\\w+|\\(|\\)|and|or|not', query)\n",
        "\n",
        "    ops = []\n",
        "    terms = []\n",
        "\n",
        "    precedence = {'not': 3, 'and': 2, 'or': 1}\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        token = tokens[i]\n",
        "\n",
        "        if token == '(':\n",
        "            ops.append(token)\n",
        "        elif token == ')':\n",
        "            while ops and ops[-1] != '(':\n",
        "                evaluate(ops, terms)\n",
        "            ops.pop()  # Remove '(' from stack\n",
        "        elif token in ('and', 'or', 'not'):\n",
        "            while (ops and ops[-1] != '(' and\n",
        "                   precedence[ops[-1]] >= precedence[token]):\n",
        "                evaluate(ops, terms)\n",
        "            ops.append(token)\n",
        "        else:\n",
        "            terms.append(get_posting(token))\n",
        "        i += 1\n",
        "    # Evaluate remaining operations\n",
        "    while ops:\n",
        "        evaluate(ops, terms)\n",
        "    # Final result\n",
        "    s = sorted(list(terms[-1]))\n",
        "    return [docID[p] for p in s]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNDZF-_Nxzqe"
      },
      "source": [
        "### Biword Indexing for Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-ArLVPqxzqf"
      },
      "outputs": [],
      "source": [
        "# Create biword index\n",
        "biword_index = {}\n",
        "for i in range(1, 42):\n",
        "    for j in range(len(doclist[i]) - 1):  # Get consecutive word pairs\n",
        "        biword = doclist[i][j] + \" \" + doclist[i][j + 1]\n",
        "        if biword not in biword_index:\n",
        "            biword_index[biword] = []\n",
        "        if i not in biword_index[biword]:\n",
        "            biword_index[biword].append(i)\n",
        "\n",
        "# Function to handle phrase queries\n",
        "def process_phrase_query(query):\n",
        "    biword_query = \" \".join(query.split())  # Join words into a phrase\n",
        "    if biword_query in biword_index:\n",
        "        return set(biword_index[biword_query])\n",
        "    return set()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dM4ZDgLxzqf"
      },
      "source": [
        "### Using Positional Index for Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE8-eF95xzqf"
      },
      "outputs": [],
      "source": [
        "# Create Positional Index\n",
        "positional_index = {}\n",
        "for i in range(1, 42):\n",
        "    for j, word in enumerate(doclist[i]):\n",
        "        if word not in positional_index:\n",
        "            positional_index[word] = {}\n",
        "        if i not in positional_index[word]:\n",
        "            positional_index[word][i] = []\n",
        "        positional_index[word][i].append(j)  # Store the position of the word in the document\n",
        "\n",
        "# Function to handle proximity queries\n",
        "def process_proximity_query(term1, term2, distance):\n",
        "    postings1 = positional_index.get(term1, {})\n",
        "    postings2 = positional_index.get(term2, {})\n",
        "    result = set()\n",
        "\n",
        "    # Check if the two terms are within the specified distance in any document\n",
        "    for doc in postings1:\n",
        "        if doc in postings2:\n",
        "            positions1 = postings1[doc]\n",
        "            positions2 = postings2[doc]\n",
        "            for pos1 in positions1:\n",
        "                for pos2 in positions2:\n",
        "                    if abs(pos1 - pos2) <= distance:\n",
        "                        result.add(doc)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVfwhCyUxzqg"
      },
      "source": [
        "### Soundex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poWbu3eqxzqg"
      },
      "outputs": [],
      "source": [
        "# Soundex function to compute Soundex code for a word\n",
        "def soundex(word):\n",
        "    word = word.lower()\n",
        "    codes = {\"a\": \"\", \"e\": \"\", \"i\": \"\", \"o\": \"\", \"u\": \"\", \"y\": \"\", \"h\": \"\", \"w\": \"\",\n",
        "             \"b\": \"1\", \"f\": \"1\", \"p\": \"1\", \"v\": \"1\",\n",
        "             \"c\": \"2\", \"g\": \"2\", \"j\": \"2\", \"k\": \"2\", \"q\": \"2\", \"s\": \"2\", \"x\": \"2\", \"z\": \"2\",\n",
        "             \"d\": \"3\", \"t\": \"3\",\n",
        "             \"l\": \"4\",\n",
        "             \"m\": \"5\", \"n\": \"5\",\n",
        "             \"r\": \"6\"}\n",
        "\n",
        "    soundex_code = word[0].upper()\n",
        "    for char in word[1:]:\n",
        "        if codes[char] != \"\":\n",
        "            if codes[char] != soundex_code[-1]:  # Avoid duplicates\n",
        "                soundex_code += codes[char]\n",
        "\n",
        "    soundex_code = soundex_code[:4].ljust(4, \"0\")  # Pad with 0s to make it 4 characters long\n",
        "    return soundex_code\n",
        "\n",
        "# Create Soundex index\n",
        "soundex_index = {}\n",
        "for word in inverted_index:\n",
        "    sdx_code = soundex(word)\n",
        "    if sdx_code not in soundex_index:\n",
        "        soundex_index[sdx_code] = []\n",
        "    soundex_index[sdx_code].append(word)\n",
        "\n",
        "# Function to handle soundex spelling matching queries with two terms\n",
        "def soundex_search(term1, term2):\n",
        "    result1 = soundex_search_single(term1)\n",
        "    result2 = soundex_search_single(term2)\n",
        "    return result1.intersection(result2)\n",
        "\n",
        "# Function to retrieve documents for a single soundex term\n",
        "def soundex_search_single(term):\n",
        "    term_sdx = soundex(term)\n",
        "    return set(inverted_index.get(word, []) for word in soundex_index.get(term_sdx, []))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwx6UXPLxzqg"
      },
      "source": [
        "### Extending Boolean Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on5vMWUyxzqh"
      },
      "outputs": [],
      "source": [
        "# def get_posting_biword(term):\n",
        "#     # Check if it's a phrase query (e.g., \"artificial intelligence\")\n",
        "#     if '\"' in term:\n",
        "#         return get_phrase_postings(term.strip('\"'))\n",
        "\n",
        "# def get_posting_proximity(term):\n",
        "#     # Check if it's a proximity query (e.g., \"word1 NEAR word2/3\")\n",
        "#     if 'near' in term:\n",
        "#         parts = term.split()\n",
        "#         return get_proximity_postings(parts[0], parts[2], int(parts[3]))\n",
        "\n",
        "# def get_posting_soundex(term):\n",
        "#     # Check if it's a misspelled name and use Soundex\n",
        "#     if term.isalpha() and len(term) > 1:\n",
        "#         return get_soundex_posting(term)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S-sgqU_xzqh"
      },
      "outputs": [],
      "source": [
        "# Extended Boolean query processing\n",
        "def extended_boolean_query(query):\n",
        "    if '\"' in query:  # Check for phrase query using quotes\n",
        "        return process_phrase_query(query.replace('\"', ''))\n",
        "    elif \"~\" in query:  # Check for proximity query\n",
        "        terms = query.split()\n",
        "        term1 = terms[0]\n",
        "        term2 = terms[2]\n",
        "        distance = int(terms[1].replace(\"~\", \"\"))\n",
        "        return process_proximity_query(term1, term2, distance)\n",
        "    elif \"soundex\" in query:  # Check for soundex query keyword\n",
        "        term = query.split()[1]\n",
        "        return soundex_search(term)\n",
        "    else:\n",
        "        return process_boolean_query(query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX-d1b4Txzqi"
      },
      "source": [
        "### Q1 Test Cases - Boolean Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxiebfUDxzqi",
        "outputId": "a66355c5-3e54-4ecd-bfce-683d1fcf0651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents matching query: ['google', 'huawei', 'messenger', 'nokia', 'operating', 'skype', 'zomato']\n",
            "Documents matching query: ['blackberry', 'flipkart', 'google', 'swiggy', 'zomato']\n"
          ]
        }
      ],
      "source": [
        "query1 = \"technology AND phone\"\n",
        "query2 = \"deliveries or foods\"\n",
        "result_docs_1 = process_boolean_query(query1)\n",
        "result_docs_2 = process_boolean_query(query2)\n",
        "print(\"Documents matching query:\", result_docs_1)\n",
        "print(\"Documents matching query:\", result_docs_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbMX3J0Mxzqi"
      },
      "source": [
        "### Q2 Test Cases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rR6C_Xhxzqj"
      },
      "source": [
        "Choices: <br>\n",
        "1. Biword <br>\n",
        "2. Proximity <br>\n",
        "3. Soundex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbmd3iVExzqj"
      },
      "source": [
        "#### Biword Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySesICxDxzqj",
        "outputId": "805027db-96b6-490a-9d1e-b2bcfd410bad"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[91], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m q2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearching engines\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m q3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownloaded apps\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m result1 \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_extended_boolean_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m result2 \u001b[38;5;241m=\u001b[39m process_extended_boolean_query(q2, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m result3 \u001b[38;5;241m=\u001b[39m process_extended_boolean_query(q3, \u001b[38;5;241m1\u001b[39m)\n",
            "Cell \u001b[1;32mIn[89], line 37\u001b[0m, in \u001b[0;36mprocess_extended_boolean_query\u001b[1;34m(query, ch)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m ops:\n\u001b[0;32m     35\u001b[0m     evaluate(ops, terms)\n\u001b[1;32m---> 37\u001b[0m result_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mterms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [docID[p] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m result_docs]\n",
            "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ],
      "source": [
        "\n",
        "# Test cases for Biword Index\n",
        "biword_queries = {\n",
        "    'q1': 'search engine',\n",
        "    'q2': 'searching engines',\n",
        "    'q3': 'downloaded apps'\n",
        "}\n",
        "\n",
        "# Example usage for Biword Index queries\n",
        "for q in biword_queries:\n",
        "    result_docs_biword = process_phrase_query(biword_queries[q])\n",
        "    print(f\"Documents matching biword query {q}: {[docID[p] for p in result_docs_biword]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x0sh0z1xzqk"
      },
      "source": [
        "#### Proximity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvtl3X-6xzqk"
      },
      "outputs": [],
      "source": [
        "# Test cases for Proximity Queries\n",
        "proximity_queries = {\n",
        "    'q1': ('easy', 'passenger', 10),\n",
        "    'q2': ('graphic', 'capabilities', 5),\n",
        "    'q3': ('graphic', 'capabilities', 10)\n",
        "}\n",
        "\n",
        "# Example usage for Proximity Queries\n",
        "for q in proximity_queries:\n",
        "    term1, term2, dist = proximity_queries[q]\n",
        "    result_docs_proximity = process_proximity_query(term1, term2, dist)\n",
        "    print(f\"Documents matching proximity query {q}: {[docID[p] for p in result_docs_proximity]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCrewxb1xzqk"
      },
      "source": [
        "#### Soundex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV-p4HjFxzql"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example usage for soundex spelling matching\n",
        "soundex_queries = {\n",
        "    'q1': ('lehri', 'stainford'),\n",
        "    'q2': ('yahu', 'dauwnloads')\n",
        "}\n",
        "\n",
        "# Example usage for Soundex Queries\n",
        "for q in soundex_queries:\n",
        "    term1, term2 = soundex_queries[q]\n",
        "    result_docs_soundex = soundex_search(term1, term2)\n",
        "    print(f\"Documents matching soundex query {q}: {[docID[p] for p in result_docs_soundex]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
